#datos <- read_csv("./train-2.csv")
datos <- read_csv("~/Desktop/GitHub/MO_Proyecto/train.csv")
library(nnet)
library(tidyverse)
#datos <- read_csv("./train-2.csv")
datos <- read_csv("~/Desktop/GitHub/MO_Proyecto/train.csv")
#datos <- read_csv("./train-2.csv")
datos <- read_csv("~/Desktop/GitHub/MO_Proyecto/train.csv")
View(datos)
pred_ml <- function(x, beta){
p <- ncol(x)
K <- length(beta)/(p+1) + 1
beta_mat <- matrix(beta, K - 1, p + 1 , byrow = TRUE)
u_beta <- exp(as.matrix(cbind(1, x)) %*% t(beta_mat))
Z <- 1 + apply(u_beta, 1, sum)
p_beta <- cbind(u_beta, 1)/Z
as.matrix(p_beta)
}
devianza_calc <- function(x, y){
dev_fun <- function(beta){
p_beta <- pred_ml(x, beta)
p <- sapply(1:nrow(x), function(i) p_beta[i, y[i]+1])
-2*sum(log(p))
}
dev_fun
}
grad_calc <- function(x_ent, y_ent){
p <- ncol(x_ent)
K <- length(unique(y_ent))
y_fact <- factor(y_ent)
# matriz de indicadoras de clase
y_dummy <-  model.matrix(~-1 + y_fact)
print(y_dummy)
salida_grad <- function(beta){
p_beta <-  pred_ml(x_ent, beta)
e_mat <-  (y_dummy  - p_beta)[, -K]
grad_out <- -2*(t(cbind(1,x_ent)) %*% e_mat)
as.numeric(grad_out)
}
salida_grad
}
descenso <- function(n, z_0, eta, h_deriv, dev_fun){
z <- matrix(0,n, length(z_0))
z[1, ] <- z_0
for(i in 1:(n-1)){
z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
if(i %% 10 == 0){
print(paste0(i, ' Devianza: ', dev_fun(z[i+1, ])))
}
}
z
}
set.seed(530)
ent <- sample_frac(datos,0.5) %>% data.frame()
prueba <- anti_join(datos,ent)
x_ent <- ent %>% select(-c(experiment,event)) %>% as.matrix
y_ent <- ent %>% select(c(event))
x_ent_s <- x_ent %>% scale()
y_ent$event[y_ent$event == "A"] <- 0
y_ent$event[y_ent$event == "B"] <- 1
y_ent$event[y_ent$event == "C"] <- 2
y_ent$event[y_ent$event == "D"] <- 3
y_ent <- unlist(y_ent) %>% as.numeric()
mod_mult <- multinom(y_ent ~ x_ent_s, data = ent, MaxNWt=100000, maxit = 0)
mod_mult
